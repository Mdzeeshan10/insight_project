{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "def convert_pdf_to_txt(pdf_path, txt_path):\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as pdf_file:\n",
    "            pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "            text = ''\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "                txt_file.write(text)\n",
    "            print(f\"PDF '{pdf_path}' converted to TXT successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting '{pdf_path}':\", e)\n",
    "\n",
    "def batch_convert_pdfs(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    for pdf_file in os.listdir(input_folder):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(input_folder, pdf_file)\n",
    "            txt_filename = os.path.splitext(pdf_file)[0] + '.txt'\n",
    "            txt_path = os.path.join(output_folder, txt_filename)\n",
    "            convert_pdf_to_txt(pdf_path, txt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF 'pdf_data\\ChatBot1 english 2.pdf' converted to TXT successfully!\n",
      "PDF 'pdf_data\\FAQ 2.pdf' converted to TXT successfully!\n"
     ]
    }
   ],
   "source": [
    "# Replace 'input_folder' with the folder containing your PDF files\n",
    "input_folder = 'pdf_data'\n",
    "\n",
    "# Replace 'output_folder' with the desired folder for the output TXT files\n",
    "output_folder = 'txt_data'\n",
    "\n",
    "batch_convert_pdfs(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = DirectoryLoader(\"./txt_data\", glob=\"*.txt\", loader_cls=TextLoader)\n",
    "loader1 = DirectoryLoader('txt_data/', glob=\"*.txt\", loader_cls=TextLoader, loader_kwargs={'autodetect_encoding': True})\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "                                       chunk_overlap=50)\n",
    "documents1 = loader1.load()\n",
    "texts = splitter.split_documents(documents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct Pdf File directory Loading\n",
    "\n",
    "\n",
    "# from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "# loader = PyPDFDirectoryLoader(\"pdf_data/\")\n",
    "# # interpret information in the documents\n",
    "# documents =  loader.load()\n",
    "\n",
    "# splitter = RecursiveCharacterTextSplitter(chunk_size=500,\n",
    "#                                           chunk_overlap=50)\n",
    "# texts = splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mpasha1\\OneDrive - Insight\\Desktop\\Llama 2 work\\Llama2 em\\llmavecenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "modules.json: 100%|██████████| 349/349 [00:00<?, ?B/s] \n",
      "c:\\Users\\mpasha1\\OneDrive - Insight\\Desktop\\Llama 2 work\\Llama2 em\\llmavecenv\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mpasha1\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<?, ?B/s] \n",
      "README.md: 100%|██████████| 10.7k/10.7k [00:00<00:00, 10.6MB/s]\n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 51.6kB/s]\n",
      "config.json: 100%|██████████| 612/612 [00:00<?, ?B/s] \n",
      "pytorch_model.bin: 100%|██████████| 90.9M/90.9M [00:17<00:00, 5.29MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 350/350 [00:00<?, ?B/s] \n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 358kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.16MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<?, ?B/s] \n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save the local database\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "db.save_local(\"faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import CTransformers\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the template we will use when prompting the AI\n",
    "template = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\"\n",
    "\n",
    "# load the language model\n",
    "llm = CTransformers(model='model/llama-2-7b-chat.ggmlv3.q8_0.bin',\n",
    "                    model_type='llama',\n",
    "                    config={'max_new_tokens': 256, 'temperature': 0.01})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load the interpreted information from the local database\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'})\n",
    "db = FAISS.load_local(\"faiss\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a version of the llm pre-loaded with the local content\n",
    "retriever = db.as_retriever(search_kwargs={'k': 2})\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=['context', 'question'])\n",
    "qa_llm = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                     chain_type='stuff',\n",
    "                                     retriever=retriever,\n",
    "                                     return_source_documents=True,\n",
    "                                     chain_type_kwargs={'prompt': prompt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mpasha1\\OneDrive - Insight\\Desktop\\Llama 2 work\\Llama2 em\\llmavecenv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamination involves the application of a range of papers available in white, ivory or pearl to protect the print from rubbing and make it tear-proof and resistant to chemical agents. It makes labels stand out and is especially suitable for high-end containers or luxury products.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \"what is lamination?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, we use Mosaic and Collage technologies for variable data printing.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" Do you print with variable data?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our printing systems allow us to be faithful to Pantone colors with a precision range that goes from 95% to 100%. To find out if your Pantone color falls within this range, contact us at info@auroflex.it\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" In my file, there are Pantone colors. Will I actually get the same reproduction during printing?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ideal formats for sending graphic files are PDF and AI. The label must be in CMYK four-color process. All RGB projects will be converted. Yes, the fonts should be converted into paths. Yes, the images should be embedded in the document with a resolution of 300dpi. No, you cannot overlap more than 4 inks.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" What characteristics should the file to be sent have?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ideal formats for sending graphic files are PDF and AI. The label must be in CMYK four-color process. All RGB projects will be converted. Yes, the fonts should be converted into paths. Yes, the images should be embedded in the document with a resolution of 300dpi. No, you cannot overlap more than 4 inks.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" What characteristics should the file to be sent have?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"It must be a PDF file with 300dpi images embedded and fonts converted into \n",
    "paths, the color space must be CMYK and in case of using Pantone colors you should \n",
    "use the correct version in relation to the support on which you intend to print. U \n",
    "for Natural papers, C for Coated papers and Plastic Films\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamination (lamination) of the label has many advantages: it protects the print from rubbing and makes the label tear -proof and resistant to chemical agents. It makes your labels stand out, making them especially suitable for high -end containers or luxury products.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" What is the advantages of lamination?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamination (lamination) of the label has many advantages: it protects the print from rubbing and makes \n",
      "the label tear -proof and resistant to chemical agents. It makes your labels stand out, making them \n",
      "especially suitable for high -end containers or luxury products. Laminated materials can produce unique \n",
      "effects, from elegant matte to silky.\n",
      "\n",
      "Please let me know if you need anything else.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" What is the advantages of lamination? and give me in point vices?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lamination provides protection to the print from rubbing and makes it tear-proof and resistant to chemical agents. It also makes labels stand out, making them especially suitable for high-end containers or luxury products.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" Lamination ke advantages kya hai?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum file size that can be printed is 20 MB.\n"
     ]
    }
   ],
   "source": [
    "# ask the AI chat about information in our local files\n",
    "prompt = \" How is the prime minster of india?\"\n",
    "output = qa_llm({'query': prompt})\n",
    "print(output[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmavecenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
